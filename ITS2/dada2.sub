# dada2.sub

universe = vanilla
log = dada2-$(Cluster).log
error = dada2-$(Cluster).err
# Specifying we need the linux version 7 (for this version of R) and Gluster access
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 7) && (Target.HasGluster == true)

# Notify by email when finished
notification = Complete
notify_user = twhitman@wisc.edu

# Specify your executable (single binary or a script that runs several
#  commands), arguments, and a files for HTCondor to store standard
#  output (or "screen output").
executable = dada2.sh
output = dada2-$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs. The last of these lines *would* be
#  used if there were any other files needed for the executable to run.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = dada2.R, ../../Creating_R_tar/R.tar.gz
# Bringing over the R installation file and the R script we'll run
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
request_cpus = 16
#16
request_memory = 60GB
#60
request_disk = 14GB
#14
# Updated memory after full run:
# Memory took 50GB (requested 200GB)
# Disk took 3-4GB (requested 15.7GB)


# Updated memory - two samples took 956 MB memory 
# 4 samples took 1600 MB memory
# 17 samples took 6000 MB memory
#  the memory needed is supposed to scale quadratically with the number of sequences
# But maybe as the samples scales, there are fewer unique sequences...
# 
#
# Updated disk - two samples used 556 MB of disk space; four samples
# used 584 MB of disk space; 17 samples used 694 MB of disk space
# Space needed should be linear...
queue
